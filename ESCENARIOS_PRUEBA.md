# ğŸ¯ Escenarios de Prueba - Iron Dome

## ğŸ“‹ BaterÃ­a Completa de Pruebas

Esta guÃ­a proporciona escenarios de prueba estructurados para evaluar diferentes aspectos del sistema Iron Dome bajo condiciones controladas.

---

## ğŸ§ª CategorÃ­as de Pruebas

### **1. Pruebas de Funcionalidad BÃ¡sica**
### **2. Pruebas de Rendimiento**
### **3. Pruebas de EstrÃ©s**
### **4. Pruebas de Robustez**
### **5. Pruebas de OptimizaciÃ³n**

---

## ğŸ”¬ 1. Pruebas de Funcionalidad BÃ¡sica

### **Test 1.1: Funcionamiento Normal**
**Objetivo:** Verificar operaciÃ³n bÃ¡sica del sistema

**ConfiguraciÃ³n:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
reload-time-param: 8
interceptor-speed-param: 2.5
missile-speed: 1.2
attack-frequency: 8
missiles-per-attack: 2
num-targets: 6
show-detection-ranges?: On
```

**Procedimiento:**
1. Ejecutar `setup`
2. Ejecutar `go` por 500 ticks
3. Observar comportamiento normal

**Resultados Esperados:**
- Tasa de Ã©xito: 85-95%
- DetecciÃ³n automÃ¡tica funcionando
- Interceptores persiguiendo objetivos
- Sin errores de ejecuciÃ³n

**Criterios de Ã‰xito:**
- âœ… Sistema detecta misiles (cambio a naranja)
- âœ… Estaciones lanzan interceptores
- âœ… IntercepciÃ³n visible con explosiones amarillas
- âœ… MÃ©tricas actualizÃ¡ndose correctamente

---

### **Test 1.2: DetecciÃ³n y Rastreo**
**Objetivo:** Verificar sistema de detecciÃ³n radar

**ConfiguraciÃ³n:**
```
num-defense-stations: 1
detection-range-param: 20
interception-range-param: 15
attack-frequency: 5
missiles-per-attack: 1
```

**Procedimiento:**
1. Lanzar misil Ãºnico con `emergency-launch-attack`
2. Observar cambio de color del misil
3. Verificar que estaciÃ³n lo rastrea

**Resultados Esperados:**
- Misil cambia de rojo a naranja al entrar en rango
- EstaciÃ³n debe mostrar actividad de rastreo

---

### **Test 1.3: CÃ¡lculo BalÃ­stico**
**Objetivo:** Verificar precisiÃ³n de intercepciÃ³n

**ConfiguraciÃ³n:**
```
interceptor-speed-param: 3.0
missile-speed: 1.0
attack-frequency: 5
```

**Procedimiento:**
1. Configurar interceptores muy rÃ¡pidos vs. misiles lentos
2. Observar tasa de intercepciÃ³n
3. Verificar que interceptores alcanzan objetivos

**Resultados Esperados:**
- Tasa de Ã©xito > 95%
- Interceptores alcanzan misiles antes del impacto

---

## âš¡ 2. Pruebas de Rendimiento

### **Test 2.1: MÃºltiples Estaciones**
**Objetivo:** Evaluar coordinaciÃ³n entre estaciones

**ConfiguraciÃ³n:**
```
num-defense-stations: 6
detection-range-param: 15
interception-range-param: 12
attack-frequency: 10
missiles-per-attack: 3
```

**Procedimiento:**
1. Ejecutar simulaciÃ³n por 1000 ticks
2. Observar distribuciÃ³n de carga entre estaciones
3. Medir eficiencia del sistema

**MÃ©tricas a Evaluar:**
- Tasa de Ã©xito con mÃºltiples estaciones
- Tiempo de respuesta promedio
- UtilizaciÃ³n de cada estaciÃ³n

**Resultados Esperados:**
- Mejora en tasa de Ã©xito vs. test 1.1
- Redundancia operacional visible
- Sin sobrecarga de estaciones individuales

---

### **Test 2.2: Escalabilidad de Amenazas**
**Objetivo:** Evaluar respuesta a incremento gradual de amenazas

**ConfiguraciÃ³n Base:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
```

**Procedimiento:**
Ejecutar 5 sub-tests incrementales:

| Sub-Test | missiles-per-attack | attack-frequency | DuraciÃ³n |
|----------|---------------------|------------------|----------|
| 2.2a | 1 | 5 | 200 ticks |
| 2.2b | 2 | 8 | 200 ticks |
| 2.2c | 3 | 10 | 200 ticks |
| 2.2d | 4 | 12 | 200 ticks |
| 2.2e | 5 | 15 | 200 ticks |

**AnÃ¡lisis:**
- Graficar tasa de Ã©xito vs. intensidad de ataque
- Identificar punto de saturaciÃ³n del sistema
- Documentar degradaciÃ³n de rendimiento

---

### **Test 2.3: OptimizaciÃ³n de Velocidades**
**Objetivo:** Encontrar balance Ã³ptimo interceptor/misil

**ConfiguraciÃ³n Variables:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
attack-frequency: 8
missiles-per-attack: 2
```

**Matriz de Pruebas:**

| Test | interceptor-speed | missile-speed | Tasa Esperada |
|------|-------------------|---------------|---------------|
| 2.3a | 2.0 | 1.0 | >90% |
| 2.3b | 2.5 | 1.2 | >85% |
| 2.3c | 3.0 | 1.5 | >80% |
| 2.3d | 2.0 | 1.8 | >60% |
| 2.3e | 1.5 | 2.0 | >40% |

**AnÃ¡lisis:**
- Determinar ratio crÃ­tico velocidad interceptor/misil
- Identificar configuraciÃ³n Ã³ptima costo-efectiva

---

## ğŸ”¥ 3. Pruebas de EstrÃ©s

### **Test 3.1: SaturaciÃ³n del Sistema**
**Objetivo:** Encontrar lÃ­mites operacionales del sistema

**ConfiguraciÃ³n:**
```
num-defense-stations: 3
detection-range-param: 12
interception-range-param: 10
interceptor-speed-param: 2.0
missile-speed: 1.5
attack-frequency: 18
missiles-per-attack: 5
```

**Procedimiento:**
1. Ejecutar por 300 ticks
2. Usar `emergency-launch-attack` cada 50 ticks
3. Observar degradaciÃ³n del sistema

**MÃ©tricas CrÃ­ticas:**
- Tasa de Ã©xito bajo estrÃ©s
- NÃºmero de amenazas simultÃ¡neas mÃ¡ximo
- Tiempo de recuperaciÃ³n del sistema

**Resultados Esperados:**
- Tasa de Ã©xito < 60%
- MÃºltiples impactos simultÃ¡neos
- Sistema saturado pero funcional

---

### **Test 3.2: Ataque Coordinado Masivo**
**Objetivo:** Simular ataque militar coordinado

**ConfiguraciÃ³n:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
missile-speed: 1.3
attack-frequency: 20
missiles-per-attack: 4
```

**Procedimiento:**
1. Lanzar 3 `emergency-launch-attack` consecutivos
2. Observar respuesta del sistema
3. Medir tiempo de recuperaciÃ³n

**Escenario:**
- 15 misiles lanzados en ~30 ticks
- EvaluaciÃ³n de priorizaciÃ³n bajo presiÃ³n extrema
- AnÃ¡lisis de efectividad vs. ataques de saturaciÃ³n

---

### **Test 3.3: Misiles de Alta Velocidad**
**Objetivo:** Evaluar respuesta a amenazas supersÃ³nicas

**ConfiguraciÃ³n:**
```
num-defense-stations: 4
detection-range-param: 20
interception-range-param: 15
interceptor-speed-param: 2.5
missile-speed: 2.0
attack-frequency: 8
missiles-per-attack: 2
```

**Procedimiento:**
1. Ejecutar simulaciÃ³n con misiles muy rÃ¡pidos
2. Observar eficiencia de intercepciÃ³n
3. Analizar tiempo de respuesta crÃ­tico

**DesafÃ­os Esperados:**
- Ventana de intercepciÃ³n muy reducida
- Necesidad de detecciÃ³n temprana
- Posible falla en intercepciÃ³n tardÃ­a

---

## ğŸ›¡ï¸ 4. Pruebas de Robustez

### **Test 4.1: Falla de Estaciones**
**Objetivo:** Evaluar redundancia y recuperaciÃ³n

**ConfiguraciÃ³n:**
```
num-defense-stations: 5
detection-range-param: 15
interception-range-param: 12
attack-frequency: 10
missiles-per-attack: 3
```

**Procedimiento:**
1. Ejecutar por 200 ticks (operaciÃ³n normal)
2. Usar `disable-random-station` 2 veces
3. Continuar por 200 ticks mÃ¡s
4. Usar `repair-all-stations`
5. Ejecutar 200 ticks finales

**AnÃ¡lisis:**
- DegradaciÃ³n de rendimiento con estaciones daÃ±adas
- Capacidad de compensaciÃ³n del sistema
- RecuperaciÃ³n post-reparaciÃ³n

**MÃ©tricas:**
- Tasa de Ã©xito: Normal vs. Degradado vs. Recuperado
- Cobertura territorial afectada
- Tiempo de adaptaciÃ³n del sistema

---

### **Test 4.2: Cobertura Territorial**
**Objetivo:** Evaluar puntos ciegos y sobreposiciÃ³n

**ConfiguraciÃ³n:**
```
num-defense-stations: 3
detection-range-param: 12
interception-range-param: 10
show-detection-ranges?: On
```

**Procedimiento:**
1. Observar visualmente Ã¡reas de cobertura
2. Identificar zonas sin protecciÃ³n
3. Lanzar ataques especÃ­ficos en puntos ciegos
4. Documentar vulnerabilidades

**AnÃ¡lisis GeogrÃ¡fico:**
- Mapear zonas de cobertura efectiva
- Identificar gaps en la defensa
- Evaluar necesidad de estaciones adicionales

---

### **Test 4.3: Ataques Direccionales**
**Objetivo:** Evaluar vulnerabilidades direccionales

**ConfiguraciÃ³n:**
```
num-defense-stations: 4
attack-frequency: 15
missiles-per-attack: 3
```

**Procedimiento:**
1. Observar patrones de lanzamiento aleatorio
2. Identificar sectores mÃ¡s vulnerables
3. Analizar efectividad por regiÃ³n

**Nota:** Los misiles se lanzan aleatoriamente, pero se puede observar patrones de efectividad segÃºn la geometrÃ­a del sistema.

---

## ğŸ¯ 5. Pruebas de OptimizaciÃ³n

### **Test 5.1: ConfiguraciÃ³n Ã“ptima**
**Objetivo:** Encontrar configuraciÃ³n de mÃ¡xima efectividad

**MetodologÃ­a:** Grid Search sistemÃ¡tico

**Variables a Optimizar:**
```
num-defense-stations: [3, 4, 5, 6]
detection-range-param: [12, 15, 18, 20]
interception-range-param: [10, 12, 15]
interceptor-speed-param: [2.0, 2.5, 3.0]
```

**Procedimiento:**
1. Ejecutar cada combinaciÃ³n por 500 ticks
2. Registrar tasa de Ã©xito promedio
3. Analizar trade-offs costo-efectividad
4. Identificar configuraciÃ³n Ã³ptima

**Matriz de Resultados:**
Documentar en tabla: ConfiguraciÃ³n â†’ Tasa de Ã‰xito â†’ Costo Relativo

---

### **Test 5.2: AnÃ¡lisis de Sensibilidad**
**Objetivo:** Identificar parÃ¡metros mÃ¡s crÃ­ticos

**ConfiguraciÃ³n Base:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
missile-speed: 1.2
```

**Procedimiento:**
Para cada parÃ¡metro, variar Â±20% y medir impacto:

| ParÃ¡metro | VariaciÃ³n | Impacto Esperado |
|-----------|-----------|------------------|
| `num-defense-stations` | 3â†’5 | Alto |
| `detection-range-param` | 12â†’18 | Alto |
| `interceptor-speed-param` | 2.0â†’3.0 | Muy Alto |
| `interception-range-param` | 10â†’14 | Medio |
| `reload-time-param` | 6â†’10 | Bajo |

**AnÃ¡lisis:**
- Ranking de importancia de parÃ¡metros
- IdentificaciÃ³n de factores crÃ­ticos
- Recomendaciones de configuraciÃ³n

---

### **Test 5.3: Eficiencia de Recursos**
**Objetivo:** Optimizar relaciÃ³n costo-beneficio

**Escenarios de Presupuesto:**

**Escenario A: Presupuesto MÃ­nimo**
```
num-defense-stations: 3
detection-range-param: 12
interception-range-param: 10
interceptor-speed-param: 2.0
```

**Escenario B: Presupuesto Medio**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
```

**Escenario C: Presupuesto Alto**
```
num-defense-stations: 6
detection-range-param: 20
interception-range-param: 15
interceptor-speed-param: 3.0
```

**AnÃ¡lisis ROI:**
- Efectividad por unidad de costo
- Punto de rendimientos decrecientes
- RecomendaciÃ³n de inversiÃ³n Ã³ptima

---

## ğŸ“Š Plantilla de Reporte de Pruebas

### **Para cada test, documentar:**

```
TEST ID: [NÃºmero de test]
FECHA: [Fecha de ejecuciÃ³n]
DURACIÃ“N: [Ticks ejecutados]
CONFIGURACIÃ“N: [ParÃ¡metros usados]

RESULTADOS:
- Tasa de Ã‰xito: [%]
- Total Misiles: [cantidad]
- Interceptados: [cantidad]
- Impactos: [cantidad]
- Amenazas MÃ¡x SimultÃ¡neas: [cantidad]

OBSERVACIONES:
- [Comportamientos notables]
- [AnomalÃ­as detectadas]
- [Patrones identificados]

CONCLUSIONES:
- [Cumplimiento de objetivos]
- [Recomendaciones]
- [PrÃ³ximos pasos]
```

---

## ğŸ¯ Casos de Uso EspecÃ­ficos

### **Caso 1: EvaluaciÃ³n de Compra**
**Escenario:** Decidir cuÃ¡ntas estaciones comprar
**Tests Recomendados:** 2.1, 4.1, 5.1, 5.3
**Objetivo:** Optimizar nÃºmero de estaciones vs. presupuesto

### **Caso 2: ConfiguraciÃ³n Operacional**
**Escenario:** Configurar sistema existente
**Tests Recomendados:** 1.1, 2.3, 5.2
**Objetivo:** Maximizar efectividad con hardware fijo

### **Caso 3: EvaluaciÃ³n de Amenazas**
**Escenario:** Prepararse para amenazas especÃ­ficas
**Tests Recomendados:** 3.1, 3.2, 3.3
**Objetivo:** Validar efectividad contra amenazas conocidas

### **Caso 4: Mantenimiento Preventivo**
**Escenario:** Planificar mantenimiento sin vulnerabilidades
**Tests Recomendados:** 4.1, 4.2
**Objetivo:** Identificar redundancias crÃ­ticas

---

## ğŸ† Benchmarks de Referencia

### **Sistema Iron Dome Real:**
- Tasa de Ã©xito reportada: ~90%
- Cobertura: ~150 kmÂ²
- Tiempo de respuesta: <15 segundos
- Costo por interceptor: ~$50,000

### **Objetivos de SimulaciÃ³n:**
- Tasa de Ã©xito objetivo: >85%
- Tiempo de respuesta: <20 ticks
- Eficiencia: >80% interceptores alcanzan objetivo
- Robustez: >70% efectividad con 1 estaciÃ³n daÃ±ada

---

## âš¡ Tests de RegresiÃ³n

### **Ejecutar antes de cambios de cÃ³digo:**

1. **Test BÃ¡sico:** 1.1 (Funcionamiento Normal)
2. **Test de EstrÃ©s:** 3.1 (SaturaciÃ³n)
3. **Test de Robustez:** 4.1 (Falla de Estaciones)

### **Criterios de AceptaciÃ³n:**
- âœ… Sin errores de ejecuciÃ³n
- âœ… Tasa de Ã©xito dentro de Â±5% de lÃ­nea base
- âœ… Todas las mÃ©tricas actualizÃ¡ndose
- âœ… Comportamiento visual correcto

---

**Â¡Estos escenarios proporcionan una evaluaciÃ³n exhaustiva del sistema Iron Dome!** ğŸ›¡ï¸ğŸ¯

*Documenta todos los resultados para crear una base de conocimiento sÃ³lida sobre el rendimiento del sistema.*
