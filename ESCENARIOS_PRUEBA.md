# üéØ Escenarios de Prueba - Iron Dome

## üìã Bater√≠a Completa de Pruebas

Esta gu√≠a proporciona escenarios de prueba estructurados para evaluar diferentes aspectos del sistema Iron Dome bajo condiciones controladas.

---

## üß™ Categor√≠as de Pruebas

### **1. Pruebas de Funcionalidad B√°sica**
### **2. Pruebas de Rendimiento**
### **3. Pruebas de Estr√©s**
### **4. Pruebas de Robustez**
### **5. Pruebas de Optimizaci√≥n**

---

## üî¨ 1. Pruebas de Funcionalidad B√°sica

### **Test 1.1: Funcionamiento Normal**
**Objetivo:** Verificar operaci√≥n b√°sica del sistema

**Configuraci√≥n:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
reload-time-param: 8
interceptor-speed-param: 2.5
missile-speed: 1.2
attack-frequency: 8
missiles-per-attack: 2
num-targets: 6
show-detection-ranges?: On
```

**Procedimiento:**
1. Ejecutar `setup`
2. Ejecutar `go` por 500 ticks
3. Observar comportamiento normal

**Resultados Esperados:**
- Tasa de √©xito: 85-95%
- Detecci√≥n autom√°tica funcionando
- Interceptores persiguiendo objetivos
- Sin errores de ejecuci√≥n

**Criterios de √âxito:**
- ‚úÖ Sistema detecta misiles (cambio a naranja)
- ‚úÖ Estaciones lanzan interceptores
- ‚úÖ Intercepci√≥n visible con explosiones amarillas
- ‚úÖ M√©tricas actualiz√°ndose correctamente

---

### **Test 1.2: Detecci√≥n y Rastreo**
**Objetivo:** Verificar sistema de detecci√≥n radar

**Configuraci√≥n:**
```
num-defense-stations: 1
detection-range-param: 20
interception-range-param: 15
attack-frequency: 5
missiles-per-attack: 1
```

**Procedimiento:**
1. Lanzar misil √∫nico con `emergency-launch-attack`
2. Observar cambio de color del misil
3. Verificar que estaci√≥n lo rastrea

**Resultados Esperados:**
- Misil cambia de rojo a naranja al entrar en rango
- Estaci√≥n debe mostrar actividad de rastreo

---

### **Test 1.3: C√°lculo Bal√≠stico**
**Objetivo:** Verificar precisi√≥n de intercepci√≥n

**Configuraci√≥n:**
```
interceptor-speed-param: 3.0
missile-speed: 1.0
attack-frequency: 5
```

**Procedimiento:**
1. Configurar interceptores muy r√°pidos vs. misiles lentos
2. Observar tasa de intercepci√≥n
3. Verificar que interceptores alcanzan objetivos

**Resultados Esperados:**
- Tasa de √©xito > 95%
- Interceptores alcanzan misiles antes del impacto

---

## ‚ö° 2. Pruebas de Rendimiento

### **Test 2.1: M√∫ltiples Estaciones**
**Objetivo:** Evaluar coordinaci√≥n entre estaciones

**Configuraci√≥n:**
```
num-defense-stations: 6
detection-range-param: 15
interception-range-param: 12
attack-frequency: 10
missiles-per-attack: 3
```

**Procedimiento:**
1. Ejecutar simulaci√≥n por 1000 ticks
2. Observar distribuci√≥n de carga entre estaciones
3. Medir eficiencia del sistema

**M√©tricas a Evaluar:**
- Tasa de √©xito con m√∫ltiples estaciones
- Tiempo de respuesta promedio
- Utilizaci√≥n de cada estaci√≥n

**Resultados Esperados:**
- Mejora en tasa de √©xito vs. test 1.1
- Redundancia operacional visible
- Sin sobrecarga de estaciones individuales

---

### **Test 2.2: Escalabilidad de Amenazas**
**Objetivo:** Evaluar respuesta a incremento gradual de amenazas

**Configuraci√≥n Base:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
```

**Procedimiento:**
Ejecutar 5 sub-tests incrementales:

| Sub-Test | missiles-per-attack | attack-frequency | Duraci√≥n |
|----------|---------------------|------------------|----------|
| 2.2a | 1 | 5 | 200 ticks |
| 2.2b | 2 | 8 | 200 ticks |
| 2.2c | 3 | 10 | 200 ticks |
| 2.2d | 4 | 12 | 200 ticks |
| 2.2e | 5 | 15 | 200 ticks |

**An√°lisis:**
- Graficar tasa de √©xito vs. intensidad de ataque
- Identificar punto de saturaci√≥n del sistema
- Documentar degradaci√≥n de rendimiento

---

### **Test 2.3: Optimizaci√≥n de Velocidades**
**Objetivo:** Encontrar balance √≥ptimo interceptor/misil

**Configuraci√≥n Variables:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
attack-frequency: 8
missiles-per-attack: 2
```

**Matriz de Pruebas:**

| Test | interceptor-speed | missile-speed | Tasa Esperada |
|------|-------------------|---------------|---------------|
| 2.3a | 2.0 | 1.0 | >90% |
| 2.3b | 2.5 | 1.2 | >85% |
| 2.3c | 3.0 | 1.5 | >80% |
| 2.3d | 2.0 | 1.8 | >60% |
| 2.3e | 1.5 | 2.0 | >40% |

**An√°lisis:**
- Determinar ratio cr√≠tico velocidad interceptor/misil
- Identificar configuraci√≥n √≥ptima costo-efectiva

---

## üî• 3. Pruebas de Estr√©s

### **Test 3.1: Saturaci√≥n del Sistema**
**Objetivo:** Encontrar l√≠mites operacionales del sistema

**Configuraci√≥n:**
```
num-defense-stations: 3
detection-range-param: 12
interception-range-param: 10
interceptor-speed-param: 2.0
missile-speed: 1.5
attack-frequency: 18
missiles-per-attack: 5
```

**Procedimiento:**
1. Ejecutar por 300 ticks
2. Usar `emergency-launch-attack` cada 50 ticks
3. Observar degradaci√≥n del sistema

**M√©tricas Cr√≠ticas:**
- Tasa de √©xito bajo estr√©s
- N√∫mero de amenazas simult√°neas m√°ximo
- Tiempo de recuperaci√≥n del sistema

**Resultados Esperados:**
- Tasa de √©xito < 60%
- M√∫ltiples impactos simult√°neos
- Sistema saturado pero funcional

---

### **Test 3.2: Ataque Coordinado Masivo**
**Objetivo:** Simular ataque militar coordinado

**Configuraci√≥n:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
missile-speed: 1.3
attack-frequency: 20
missiles-per-attack: 4
```

**Procedimiento:**
1. Lanzar 3 `emergency-launch-attack` consecutivos
2. Observar respuesta del sistema
3. Medir tiempo de recuperaci√≥n

**Escenario:**
- 15 misiles lanzados en ~30 ticks
- Evaluaci√≥n de priorizaci√≥n bajo presi√≥n extrema
- An√°lisis de efectividad vs. ataques de saturaci√≥n

---

### **Test 3.3: Misiles de Alta Velocidad**
**Objetivo:** Evaluar respuesta a amenazas supers√≥nicas

**Configuraci√≥n:**
```
num-defense-stations: 4
detection-range-param: 20
interception-range-param: 15
interceptor-speed-param: 2.5
missile-speed: 2.0
attack-frequency: 8
missiles-per-attack: 2
```

**Procedimiento:**
1. Ejecutar simulaci√≥n con misiles muy r√°pidos
2. Observar eficiencia de intercepci√≥n
3. Analizar tiempo de respuesta cr√≠tico

**Desaf√≠os Esperados:**
- Ventana de intercepci√≥n muy reducida
- Necesidad de detecci√≥n temprana
- Posible falla en intercepci√≥n tard√≠a

---

## üõ°Ô∏è 4. Pruebas de Robustez

### **Test 4.1: Falla de Estaciones**
**Objetivo:** Evaluar redundancia y recuperaci√≥n

**Configuraci√≥n:**
```
num-defense-stations: 5
detection-range-param: 15
interception-range-param: 12
attack-frequency: 10
missiles-per-attack: 3
```

**Procedimiento:**
1. Ejecutar por 200 ticks (operaci√≥n normal)
2. Usar `disable-random-station` 2 veces
3. Continuar por 200 ticks m√°s
4. Usar `repair-all-stations`
5. Ejecutar 200 ticks finales

**An√°lisis:**
- Degradaci√≥n de rendimiento con estaciones da√±adas
- Capacidad de compensaci√≥n del sistema
- Recuperaci√≥n post-reparaci√≥n

**M√©tricas:**
- Tasa de √©xito: Normal vs. Degradado vs. Recuperado
- Cobertura territorial afectada
- Tiempo de adaptaci√≥n del sistema

---

### **Test 4.2: Cobertura Territorial**
**Objetivo:** Evaluar puntos ciegos y sobreposici√≥n

**Configuraci√≥n:**
```
num-defense-stations: 3
detection-range-param: 12
interception-range-param: 10
show-detection-ranges?: On
```

**Procedimiento:**
1. Observar visualmente √°reas de cobertura
2. Identificar zonas sin protecci√≥n
3. Lanzar ataques espec√≠ficos en puntos ciegos
4. Documentar vulnerabilidades

**An√°lisis Geogr√°fico:**
- Mapear zonas de cobertura efectiva
- Identificar gaps en la defensa
- Evaluar necesidad de estaciones adicionales

---

### **Test 4.3: Ataques Direccionales**
**Objetivo:** Evaluar vulnerabilidades direccionales

**Configuraci√≥n:**
```
num-defense-stations: 4
attack-frequency: 15
missiles-per-attack: 3
```

**Procedimiento:**
1. Observar patrones de lanzamiento aleatorio
2. Identificar sectores m√°s vulnerables
3. Analizar efectividad por regi√≥n

**Nota:** Los misiles se lanzan aleatoriamente, pero se puede observar patrones de efectividad seg√∫n la geometr√≠a del sistema.

---

## üéØ 5. Pruebas de Optimizaci√≥n

### **Test 5.1: Configuraci√≥n √ìptima**
**Objetivo:** Encontrar configuraci√≥n de m√°xima efectividad

**Metodolog√≠a:** Grid Search sistem√°tico

**Variables a Optimizar:**
```
num-defense-stations: [3, 4, 5, 6]
detection-range-param: [12, 15, 18, 20]
interception-range-param: [10, 12, 15]
interceptor-speed-param: [2.0, 2.5, 3.0]
```

**Procedimiento:**
1. Ejecutar cada combinaci√≥n por 500 ticks
2. Registrar tasa de √©xito promedio
3. Analizar trade-offs costo-efectividad
4. Identificar configuraci√≥n √≥ptima

**Matriz de Resultados:**
Documentar en tabla: Configuraci√≥n ‚Üí Tasa de √âxito ‚Üí Costo Relativo

---

### **Test 5.2: An√°lisis de Sensibilidad**
**Objetivo:** Identificar par√°metros m√°s cr√≠ticos

**Configuraci√≥n Base:**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
missile-speed: 1.2
```

**Procedimiento:**
Para cada par√°metro, variar ¬±20% y medir impacto:

| Par√°metro | Variaci√≥n | Impacto Esperado |
|-----------|-----------|------------------|
| `num-defense-stations` | 3‚Üí5 | Alto |
| `detection-range-param` | 12‚Üí18 | Alto |
| `interceptor-speed-param` | 2.0‚Üí3.0 | Muy Alto |
| `interception-range-param` | 10‚Üí14 | Medio |
| `reload-time-param` | 6‚Üí10 | Bajo |

**An√°lisis:**
- Ranking de importancia de par√°metros
- Identificaci√≥n de factores cr√≠ticos
- Recomendaciones de configuraci√≥n

---

### **Test 5.3: Eficiencia de Recursos**
**Objetivo:** Optimizar relaci√≥n costo-beneficio

**Escenarios de Presupuesto:**

**Escenario A: Presupuesto M√≠nimo**
```
num-defense-stations: 3
detection-range-param: 12
interception-range-param: 10
interceptor-speed-param: 2.0
```

**Escenario B: Presupuesto Medio**
```
num-defense-stations: 4
detection-range-param: 15
interception-range-param: 12
interceptor-speed-param: 2.5
```

**Escenario C: Presupuesto Alto**
```
num-defense-stations: 6
detection-range-param: 20
interception-range-param: 15
interceptor-speed-param: 3.0
```

**An√°lisis ROI:**
- Efectividad por unidad de costo
- Punto de rendimientos decrecientes
- Recomendaci√≥n de inversi√≥n √≥ptima

---

## üìä Plantilla de Reporte de Pruebas

### **Para cada test, documentar:**

```
TEST ID: [N√∫mero de test]
FECHA: [Fecha de ejecuci√≥n]
DURACI√ìN: [Ticks ejecutados]
CONFIGURACI√ìN: [Par√°metros usados]

RESULTADOS:
- Tasa de √âxito: [%]
- Total Misiles: [cantidad]
- Interceptados: [cantidad]
- Impactos: [cantidad]
- Amenazas M√°x Simult√°neas: [cantidad]

OBSERVACIONES:
- [Comportamientos notables]
- [Anomal√≠as detectadas]
- [Patrones identificados]

CONCLUSIONES:
- [Cumplimiento de objetivos]
- [Recomendaciones]
- [Pr√≥ximos pasos]
```

---

## üéØ Casos de Uso Espec√≠ficos

### **Caso 1: Evaluaci√≥n de Compra**
**Escenario:** Decidir cu√°ntas estaciones comprar
**Tests Recomendados:** 2.1, 4.1, 5.1, 5.3
**Objetivo:** Optimizar n√∫mero de estaciones vs. presupuesto

### **Caso 2: Configuraci√≥n Operacional**
**Escenario:** Configurar sistema existente
**Tests Recomendados:** 1.1, 2.3, 5.2
**Objetivo:** Maximizar efectividad con hardware fijo

### **Caso 3: Evaluaci√≥n de Amenazas**
**Escenario:** Prepararse para amenazas espec√≠ficas
**Tests Recomendados:** 3.1, 3.2, 3.3
**Objetivo:** Validar efectividad contra amenazas conocidas

### **Caso 4: Mantenimiento Preventivo**
**Escenario:** Planificar mantenimiento sin vulnerabilidades
**Tests Recomendados:** 4.1, 4.2
**Objetivo:** Identificar redundancias cr√≠ticas

---

## üèÜ Benchmarks de Referencia

### **Sistema Iron Dome Real:**
- Tasa de √©xito reportada: ~90%
- Cobertura: ~150 km¬≤
- Tiempo de respuesta: <15 segundos
- Costo por interceptor: ~$50,000

### **Objetivos de Simulaci√≥n:**
- Tasa de √©xito objetivo: >85%
- Tiempo de respuesta: <20 ticks
- Eficiencia: >80% interceptores alcanzan objetivo
- Robustez: >70% efectividad con 1 estaci√≥n da√±ada

---

## ‚ö° Tests de Regresi√≥n

### **Ejecutar antes de cambios de c√≥digo:**

1. **Test B√°sico:** 1.1 (Funcionamiento Normal)
2. **Test de Estr√©s:** 3.1 (Saturaci√≥n)
3. **Test de Robustez:** 4.1 (Falla de Estaciones)

### **Criterios de Aceptaci√≥n:**
- ‚úÖ Sin errores de ejecuci√≥n
- ‚úÖ Tasa de √©xito dentro de ¬±5% de l√≠nea base
- ‚úÖ Todas las m√©tricas actualiz√°ndose
- ‚úÖ Comportamiento visual correcto

---

**¬°Estos escenarios proporcionan una evaluaci√≥n exhaustiva del sistema Iron Dome!** üõ°Ô∏èüéØ

*Documenta todos los resultados para crear una base de conocimiento s√≥lida sobre el rendimiento del sistema.*
